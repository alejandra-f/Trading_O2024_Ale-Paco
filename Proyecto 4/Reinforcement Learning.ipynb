{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bc8eb7",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a9972",
   "metadata": {},
   "source": [
    "**Created by:**\n",
    "\n",
    "Alejandra Elizabeth Figueroa Arellano\n",
    "\n",
    "Juan Francisco Cruz Sánchez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d298c61",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This report describes the design of a simulated trading environment for training reinforcement learning agents. \n",
    "\n",
    "The primary objective is to create an environment where agents can learn and apply trading strategies to maximize portfolio returns while minimizing risk and losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9eeb8",
   "metadata": {},
   "source": [
    "Financial markets are complex and highly dynamic environments where decision making requires extensive analysis and adaptability. Reinforcement learning (RL) offers a promising approach to tackle this complexity by enabling agents to learn from interactions with a simulated trading environment, so this project focuses on developing a robust trading environment that supports sequential decision-making for RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c654ed8",
   "metadata": {},
   "source": [
    "### Structure\n",
    "\n",
    "This report is divided into the following sections:\n",
    "\n",
    "- Description of the trading environment components.\n",
    "- Explanation of the state space, action space, and reward system.\n",
    "- Discussion of the key methods (reset, step, render).\n",
    "- Data generation process and its integration with the environment.\n",
    "- Next steps and summary of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8ce9c",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756bccf",
   "metadata": {},
   "source": [
    "## Description of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa391bb",
   "metadata": {},
   "source": [
    "### Action space\n",
    "\n",
    "The action space defines the set of actions the agent can take at any given time. \n",
    "\n",
    "In this environment, the action space is discrete and consists of three actions:\n",
    "\n",
    "**- Hold (0):** the agent decides to take no action, maintaining its current portfolio position.\n",
    "\n",
    "**- Buy (1):** the agent purchases a specific quantity of the asset, increasing its position.\n",
    "\n",
    "**- Sell (2):** the agent sells a specific quantity of the asset, reducing its position.\n",
    "\n",
    "These actions provide the agent with the fundamental tools to manage its portfolio actively, they represent the essential decisions required in any trading scenario, enabling the agent to adapt its strategy based on market conditions and portfolio performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c2405",
   "metadata": {},
   "source": [
    "### State space\n",
    "\n",
    "The state space encompasses all the relevant information that the agent observes at each time step to make informed decisions. \n",
    "\n",
    "In this environment, the state space includes:\n",
    "\n",
    "**- Current price of the asset.**\n",
    "\n",
    "**- Technical indicators:** metrics such as moving averages, relative strength index (RSI), and bollinger bands to provide insights into market trends.\n",
    "\n",
    "**- Current position.**\n",
    "\n",
    "**- Cash balance.**\n",
    "\n",
    "**- Time step:** the current step in the episode, indicating progress in the trading sequence.\n",
    "\n",
    "The state space provides a comprehensive view of the trading environment, by combining market data (price and indicators) with internal data (position and cash balance), the agent can evaluate both the market conditions and its trading capacity to make optimal decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a464c8",
   "metadata": {},
   "source": [
    "### Reward system\n",
    "\n",
    "The reward system incentivizes the agent to adopt profitable trading strategies while avoiding poor decisions and excessive inactivity. \n",
    "\n",
    "The design includes:\n",
    "\n",
    "**- Positive rewards for profitable trades:** the agent is rewarded for increasing the portfolio value.\n",
    "\n",
    "**- Negative rewards for unprofitable trades:** the agent is penalized for trades that decrease the portfolio value.\n",
    "\n",
    "**- Small negative rewards for holding:** to discourage the agent from remaining idle, a small penalty is applied when the agent decides to hold.\n",
    "\n",
    "The reward system balances short-term and long-term profitability. Positive rewards encourage the agent to identify and exploit profitable opportunities, while penalties for holding ensure that the agent doesn't bypass decision-making to avoid risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f72e41",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b0f4f",
   "metadata": {},
   "source": [
    "## Methods of our TradingEnv class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad164c37",
   "metadata": {},
   "source": [
    "### reset ()\n",
    "\n",
    "The reset method initializes the environment to its starting state at the beginning of each episode. \n",
    "\n",
    "It performs the following:\n",
    "\n",
    "**- Resets the agent's cash balance, position, and portfolio value to default values.**\n",
    "\n",
    "**- Sets the time step counter to zero.**\n",
    "\n",
    "**- Returns the initial state of the environment.**\n",
    "\n",
    "This method ensures that every episode starts under consistent conditions, allowing for fair evaluation and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4a53f",
   "metadata": {},
   "source": [
    "### step (action)\n",
    "\n",
    "The step method executes the action selected by the agent and updates the environment. \n",
    "\n",
    "It performs the following:\n",
    "\n",
    "**- Adjusts the state based on the agent’s action** (buying, selling, or holding).\n",
    "\n",
    "**- Calculates the reward associated with the action.**\n",
    "\n",
    "**- Checks whether the episode has ended** (end of data or portfolio liquidation).\n",
    "\n",
    "**- Returns the next state, reward, a boolean indicating whether the episode is done, and additional diagnostic information.**\n",
    "\n",
    "This method enables sequential decision-making, allowing the agent to learn through interactions with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f51603",
   "metadata": {},
   "source": [
    "### render ()\n",
    "\n",
    "The render method provides a visual or textual representation of the current state of the environment. \n",
    "\n",
    "It includes:\n",
    "\n",
    "**- Displaying the current balance, position, profit, and other performance metrics.**\n",
    "\n",
    "**- Optionally providing a graphical representation of the portfolio’s performance.**\n",
    "\n",
    "This method helps monitor the agent’s behavior and performance during training and evaluation, enabling debugging and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88ea5e",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c0dde",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5086e",
   "metadata": {},
   "source": [
    "The following steps were taken to prepare the data:\n",
    "\n",
    "- Download 10 years of historical stock data were collected and divided into 10 different datasets, each covering one year.\n",
    "\n",
    "- Generate additional scenarios: using the data, 1,000 additional scenarios were created by introducing random noise and variations to simulate different market conditions.\n",
    "\n",
    "- Data preprocessing: the data was cleaned, normalized, and transformed into a suitable format for training.\n",
    "\n",
    "**Integration with the environment**\n",
    "\n",
    "The datasets serve as the market data for the trading environment, at the beginning of each episode, one dataset is loaded, and the agent interacts with this simulated market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad876f9d",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "\n",
    "- Implementing additional features, such as transaction costs and market impact, to enhance realism.\n",
    "\n",
    "- Testing various reinforcement learning algorithms with this environment to compare performance.\n",
    "\n",
    "- Extending the environment to handle multiple assets and portfolio optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e1e89",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cac77",
   "metadata": {},
   "source": [
    "The development of this trading environment provides a robust platform for training reinforcement learning agents in a realistic and dynamic setting, the well defined action space, state space, and reward system encourage the agent to learn effective trading strategies. \n",
    "\n",
    "By using historical market data and simulating various scenarios, this environment ensures comprehensive training for the agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
